From 1765b93df77da62b1e6c76fcbc07292fe73203e2 Mon Sep 17 00:00:00 2001
From: Jianxiong Pan <jianxiong.pan@amlogic.com>
Date: Sat, 3 Sep 2022 11:26:45 +0800
Subject: [PATCH 43/95] mm: For movable memory, using CMA first [1/1]

PD#SWPL-92714

Problem:
There is still much cma memory when there is no normal memory

Solution:
CMA first after the first watermark overrun

Verify:
AH212

Change-Id: I88477b7d2d70b56f9f9ea2b1b09d287b3af07d2c
Signed-off-by: Jianxiong Pan <jianxiong.pan@amlogic.com>
---
 arch/arm/mm/dma-mapping.c |  4 ++
 drivers/usb/host/xhci.c   |  4 ++
 mm/internal.h             |  3 ++
 mm/page_alloc.c           | 91 +++++++++++++++++++++++++++++++++++++++
 4 files changed, 102 insertions(+)

diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c
index 4b61541853ea..f028deeec940 100644
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@ -709,6 +709,10 @@ static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
 	allowblock = gfpflags_allow_blocking(gfp);
 	cma = allowblock ? dev_get_cma_area(dev) : false;
 
+	#ifdef CONFIG_AMLOGIC_CMA
+	if (!!(gfp & __GFP_NO_CMA))
+		cma = false;
+	#endif
 	if (cma)
 		buf->allocator = &cma_allocator;
 	else if (is_coherent)
diff --git a/drivers/usb/host/xhci.c b/drivers/usb/host/xhci.c
index a982b5346764..6e07c6186095 100644
--- a/drivers/usb/host/xhci.c
+++ b/drivers/usb/host/xhci.c
@@ -2053,7 +2053,11 @@ int xhci_add_endpoint(struct usb_hcd *hcd, struct usb_device *udev,
 	 * process context, not interrupt context (or so documenation
 	 * for usb_set_interface() and usb_set_configuration() claim).
 	 */
+	#ifdef CONFIG_AMLOGIC_CMA
+	if (xhci_endpoint_init(xhci, virt_dev, udev, ep, GFP_NOIO | __GFP_NO_CMA) < 0) {
+	#else
 	if (xhci_endpoint_init(xhci, virt_dev, udev, ep, GFP_NOIO) < 0) {
+	#endif
 		dev_dbg(&udev->dev, "%s - could not initialize ep %#x\n",
 				__func__, ep->desc.bEndpointAddress);
 		return -ENOMEM;
diff --git a/mm/internal.h b/mm/internal.h
index 5a2facd7279f..1262237e2053 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -606,6 +606,9 @@ unsigned int reclaim_clean_pages_from_list(struct zone *zone,
 #define ALLOC_NOFRAGMENT	  0x0
 #endif
 #define ALLOC_KSWAPD		0x800 /* allow waking of kswapd, __GFP_KSWAPD_RECLAIM set */
+#ifdef CONFIG_AMLOGIC_CMA
+#define ALLOC_MOVABLE_USE_CMA_FIRST	0x1000 /* preferred to allocate from cma for movable */
+#endif
 
 enum ttu_flags;
 struct tlbflush_unmap_batch;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index d8135b7e4867..c3c662a0542d 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -3093,6 +3093,18 @@ __rmqueue(struct zone *zone, unsigned int order, int migratetype,
 {
 	struct page *page;
 
+#ifdef CONFIG_AMLOGIC_CMA
+	/* use CMA first */
+	if (migratetype == MIGRATE_MOVABLE && alloc_flags & ALLOC_MOVABLE_USE_CMA_FIRST) {
+		page = __rmqueue_cma_fallback(zone, order);
+		if (page) {
+			trace_mm_page_alloc_zone_locked(page, order,
+							MIGRATE_CMA);
+			return page;
+		}
+	}
+#endif /* CONFIG_AMLOGIC_CMA */
+
 retry:
 	page = __rmqueue_smallest(zone, order, migratetype);
 
@@ -3105,6 +3117,31 @@ __rmqueue(struct zone *zone, unsigned int order, int migratetype,
 	return page;
 }
 
+#ifdef CONFIG_AMLOGIC_CMA
+/*
+ * get page but not cma
+ */
+static struct page *rmqueue_no_cma(struct zone *zone, unsigned int order,
+				   int migratetype, unsigned int alloc_flags)
+{
+	struct page *page;
+
+	spin_lock(&zone->lock);
+retry:
+	page = __rmqueue_smallest(zone, order, migratetype);
+	if (unlikely(!page)) {
+		if (!page && __rmqueue_fallback(zone, order, migratetype, alloc_flags))
+			goto retry;
+	}
+	WARN_ON(page && is_migrate_cma(get_pcppage_migratetype(page)));
+	if (page)
+		__mod_zone_page_state(zone, NR_FREE_PAGES, -(1 << order));
+
+	spin_unlock(&zone->lock);
+	return page;
+}
+#endif /* CONFIG_AMLOGIC_CMA */
+
 #ifdef CONFIG_CMA
 static struct page *__rmqueue_cma(struct zone *zone, unsigned int order,
 				  int migratetype,
@@ -3786,6 +3823,12 @@ struct page *__rmqueue_pcplist(struct zone *zone, unsigned int order,
 {
 	struct page *page = NULL;
 	struct list_head *list = NULL;
+#ifdef CONFIG_AMLOGIC_CMA
+	bool cma = can_use_cma(gfp_flags);
+
+	if (cma)
+		alloc_flags |= ALLOC_MOVABLE_USE_CMA_FIRST;
+#endif
 
 	do {
 		/* First try to get CMA pages */
@@ -3803,6 +3846,48 @@ struct page *__rmqueue_pcplist(struct zone *zone, unsigned int order,
 		}
 
 		page = list_first_entry(list, struct page, lru);
+	#ifdef CONFIG_AMLOGIC_CMA
+		/*
+		 * USING CMA FIRST POLICY situations:
+		 * 1. CMA pages may return to pcp and allocated next
+		 *    but gfp mask is not suitable for CMA;
+		 * 2. MOVABLE pages may return to pcp and allocated next
+		 *    but gfp mask is suitable for CMA
+		 *
+		 * For 1, we should replace a none-CMA page
+		 * For 2, we should replace with a cma page
+		 * before page is deleted from PCP list.
+		 */
+		if (!cma && is_migrate_cma_page(page)) {
+			/* case 1 */
+			page = rmqueue_no_cma(zone, 0, migratetype, alloc_flags);
+			if (page) {
+				check_new_pcp(page);
+				return page;
+			} else {
+				return NULL;
+			}
+		} else if ((migratetype == MIGRATE_MOVABLE) &&
+			   (get_pcppage_migratetype(page) != MIGRATE_CMA) &&
+			   cma) {
+			struct page *t;
+
+			spin_lock(&zone->lock);
+			t = __rmqueue_cma_fallback(zone, 0);
+			/* can't alloc cma pages or not ready */
+			if (!t || check_new_pcp(page)) {
+				spin_unlock(&zone->lock);
+				goto use_pcp;
+			}
+			page = t;
+			__mod_zone_freepage_state(zone, -(1),
+						  get_pcppage_migratetype(t));
+			spin_unlock(&zone->lock);
+			check_new_pcp(page);
+			return page;
+		}
+use_pcp:
+	#endif /* CONFIG_AMLOGIC_CMA */
 		list_del(&page->lru);
 		pcp->count -= 1 << order;
 	} while (check_new_pcp(page));
@@ -3849,6 +3934,12 @@ struct page *rmqueue(struct zone *preferred_zone,
 {
 	unsigned long flags;
 	struct page *page;
+#ifdef CONFIG_AMLOGIC_CMA
+	bool cma = can_use_cma(gfp_flags);
+
+	if (cma)
+		alloc_flags |= ALLOC_MOVABLE_USE_CMA_FIRST;
+#endif
 
 	if (likely(pcp_allowed_order(order))) {
 		page = rmqueue_pcplist(preferred_zone, zone, order,
-- 
2.25.1

